{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e155dda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os,glob\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from PIL import Image\n",
    "from osgeo import gdal\n",
    "import requests\n",
    "import gzip\n",
    "import shutil\n",
    "import rioxarray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3035911e",
   "metadata": {},
   "source": [
    "# Downloading and preparing MODIS inundation maps\n",
    "this loads tif files for individual days, merges them and converts to netcdf format for easier ingestion in other scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7f88301",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModisDates(_startDate, _endDate):\n",
    "    #returns pandas datetimeindex\n",
    "    _dataJDays=range(1,365,8)\n",
    "    _allDays=pd.date_range(_startDate,_endDate,freq=\"D\")\n",
    "    return _allDays[np.in1d([x.timetuple().tm_yday for x in _allDays],_dataJDays)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6574c55c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking data between 2022-08-09 06:43:37.806256 and 2022-10-18 06:43:37.806256\n",
      "checking if http://www.okavangodata.ub.bw is up\n",
      "checking 2022-08-13 06:43:37.806256\n",
      "file A2022225.flood.tif exists locally. skipping...\n",
      "checking 2022-08-21 06:43:37.806256\n",
      "file A2022233.flood.tif exists locally. skipping...\n",
      "checking 2022-08-29 06:43:37.806256\n",
      "file A2022241.flood.tif exists locally. skipping...\n",
      "checking 2022-09-06 06:43:37.806256\n",
      "file A2022249.flood.tif exists locally. skipping...\n",
      "checking 2022-09-14 06:43:37.806256\n",
      "file A2022257.flood.tif exists locally. skipping...\n",
      "checking 2022-09-22 06:43:37.806256\n",
      "file A2022265.flood.tif exists locally. skipping...\n",
      "checking 2022-09-30 06:43:37.806256\n",
      "file A2022273.flood.tif exists locally. skipping...\n",
      "checking 2022-10-08 06:43:37.806256\n",
      "Downloading A2022281.flood.tif from http://www.okavangodata.ub.bw/modis/products_okavango/ into ../data/flood//tif/\n",
      "file A2022281.flood.tif on http://www.okavangodata.ub.bw/modis/products_okavango/ does not exist. skipping...\n",
      "checking 2022-10-16 06:43:37.806256\n",
      "Downloading A2022289.flood.tif from http://www.okavangodata.ub.bw/modis/products_okavango/ into ../data/flood//tif/\n",
      "file A2022289.flood.tif on http://www.okavangodata.ub.bw/modis/products_okavango/ does not exist. skipping...\n"
     ]
    }
   ],
   "source": [
    "#endDate should be either date as YYYY-MM-DD or \"today\"\n",
    "endDate=\"2020-02-01\"\n",
    "endDate=\"today\"\n",
    "\n",
    "#either date as YYYY-MM-DD or number of days before the endDate\n",
    "startDate=\"2020-01-01\"\n",
    "startDate=70\n",
    "\n",
    "remoteserver=\"http://www.okavangodata.ub.bw\"\n",
    "remotedir=\"{}/modis/products_okavango/\".format(remoteserver)\n",
    "#http://www.okavangodata.ub.bw/modis/products_okavango/A2022249.flood.tif\n",
    "\n",
    "datadir=\"../data/flood/\"\n",
    "tifdir=datadir+\"/tif/\"\n",
    "filenamepattern=\"A{}.flood.tif\"\n",
    "mergedfile=\"flood_modis_merged.nc\"\n",
    "\n",
    "verbose=True\n",
    "\n",
    "##########################################################################################################\n",
    "try:\n",
    "    endDate=datetime.datetime.strptime(endDate, \"%Y-%m-%d\")\n",
    "except:\n",
    "    endDate=datetime.datetime.today() #today\n",
    "\n",
    "try:\n",
    "    startDate=datetime.datetime.strptime(startDate, \"%Y-%m-%d\")\n",
    "except:\n",
    "    if not isinstance(startDate,int):\n",
    "        startDate=30\n",
    "    startDate=endDate - datetime.timedelta(startDate)\n",
    "\n",
    "if endDate<=startDate:\n",
    "    print(\"startDate has to be before endDate. You got: \\nstartDate:{} \\nendDate:{}. \\nExiting...\".format(startDate,endDate))\n",
    "    sys.exit()\n",
    "\n",
    "print(\"checking data between {} and {}\".format(startDate,endDate))\n",
    "\n",
    "dates2check=getModisDates(startDate,endDate)\n",
    "if len(dates2check)==0:\n",
    "    print(\"There are no data dates between startDate and endDate. You got: \\nstartDate:{} \\nendDate:{}. \\nExiting...\".format(startDate,endDate))\n",
    "    sys.exit()\n",
    "    \n",
    "count=0\n",
    "update=False\n",
    "\n",
    "#checking if server is up\n",
    "print(\"checking if {} is up\".format(remoteserver))\n",
    "cont=False\n",
    "try:\n",
    "    response=requests.head(remoteserver,timeout=30)\n",
    "    response.raise_for_status()\n",
    "    cont=True\n",
    "except requests.exceptions.HTTPError as errh:\n",
    "    print (\"Http Error:\",errh)\n",
    "except requests.exceptions.ConnectionError as errc:\n",
    "    print (\"Error Connecting:\",errc)\n",
    "except requests.exceptions.Timeout as errt:\n",
    "    print (\"Timeout Error:\",errt)\n",
    "except requests.exceptions.RequestException as err:\n",
    "    print (\"OOps: Something Else\",err)\n",
    "    \n",
    "if cont:\n",
    "    #server is up\n",
    "    for date in dates2check:\n",
    "        print(\"checking {}\".format(date))\n",
    "        file=filenamepattern.format(date.strftime(\"%Y%j\"))\n",
    "        filepath=tifdir+\"/\"+file\n",
    "        if os.path.exists(filepath):\n",
    "            if verbose:\n",
    "                print(\"file {} exists locally. skipping...\".format(file))\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(\"Downloading {} from {} into {}\".format(file,remotedir,tifdir))\n",
    "            url=remotedir+\"/\"+file\n",
    "            response=requests.head(url)\n",
    "            if response.status_code!=200:\n",
    "                if verbose:\n",
    "                    print(\"file {} on {} does not exist. skipping...\".format(file,remotedir))\n",
    "            else:\n",
    "                with requests.get(url, stream=True) as r:\n",
    "                    r.raise_for_status()\n",
    "                    with open(filepath, 'wb') as f:\n",
    "                        for chunk in r.iter_content(chunk_size=8192): \n",
    "                            f.write(chunk)\n",
    "\n",
    "                localfilesize=os.stat(filepath)[6]\n",
    "                remotefilesize=int(response.headers['content-length'])\n",
    "                if verbose:\n",
    "                    print (\"downloaded file size: {}\".format(localfilesize))\n",
    "                    print (\"expected file size: {}\".format(remotefilesize))\n",
    "                if localfilesize != remotefilesize:\n",
    "                    if verbose:\n",
    "                        print(\"something went wrong. removing downloaded file\")\n",
    "                    os.rename(filepath, filepath+\".fail\")\n",
    "                else:\n",
    "                    count=count+1\n",
    "                    if verbose:\n",
    "                        print(\"download successful\")\n",
    "else:\n",
    "    print(\"server {} is down.\".format(remoteserver))\n",
    "    update=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f98a8710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 0 new files. Updating merged file\n",
      "processing 711 files in ../data/flood//tif/\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "Found 711 files covering period between 2000-04-30 and 2022-09-30\n",
      "writing netcdf file: ../data/flood//flood_modis_merged.nc\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "update=True\n",
    "if count>0 or update:\n",
    "    print(\"Downloaded {} new files. Updating merged file\".format(count))\n",
    "\n",
    "    files=glob.glob(tifdir+\"/*flood.tif\")\n",
    "\n",
    "    print(\"processing {} files in {}\".format(len(files), tifdir))\n",
    "    i=0\n",
    "    for file in np.sort(files):\n",
    "        i=i+1\n",
    "        if i%100==0:\n",
    "            print(i)\n",
    "        filedatestr=os.path.basename(file)[1:8]\n",
    "        filedate=datetime.datetime.strptime(filedatestr, '%Y%j')\n",
    "        im=Image.open(file)\n",
    "        data=np.array(im)\n",
    "        if i==1:\n",
    "            dates=[filedate]\n",
    "            alldata=data[:,:,0:1]\n",
    "            ds = gdal.Open(file)\n",
    "            nx = ds.RasterXSize\n",
    "            ny = ds.RasterYSize\n",
    "            xmin,xsize,tmp,ymin,tmp,ysize=ds.GetGeoTransform()\n",
    "            lons=np.linspace(xmin,xmin+xsize*nx,num=nx)\n",
    "            lats=np.linspace(ymin,ymin+ysize*ny,num=ny)\n",
    "        else:\n",
    "            dates=dates+[filedate]\n",
    "            alldata=np.append(alldata,data[:,:,0:1],2)\n",
    "        im.close()\n",
    "    firstdatestr,lastdatestr=datetime.datetime.strftime(dates[0],\"%Y-%m-%d\"),datetime.datetime.strftime(dates[-1],\"%Y-%m-%d\")\n",
    "\n",
    "    print(\"Found {} files covering period between {} and {}\".format(len(dates),firstdatestr,lastdatestr))\n",
    "\n",
    "    #ordering axes in the array so that is has the standard time,lat,lon\n",
    "    alldata=alldata.swapaxes(0,2)\n",
    "    alldata=alldata.swapaxes(1,2)\n",
    "\n",
    "    #recoding\n",
    "    #tif files contain only 3 (0 for flooded, 255 for not flooded, and 127 for unclassified)\n",
    "    # this is recoded here to 1 for flooded,0 for not flooded and np.nan for unclassified\n",
    "    alldata[alldata==0]=1\n",
    "    alldata[alldata==255]=0\n",
    "    alldata[alldata==127]=2\n",
    "\n",
    "    ds = xr.Dataset(\n",
    "        {\"flood\": ((\"time\", \"latitude\",\"longitude\"), alldata)},\n",
    "        coords={\n",
    "            \"longitude\": lons,\n",
    "            \"latitude\": lats,\n",
    "            \"time\": dates,\n",
    "        },\n",
    "    )\n",
    "    ds[\"latitude\"].attrs = {\"units\":\"degrees_north\",'standard_name':\"latitude\",'axis':\"Y\"}\n",
    "    ds[\"longitude\"].attrs = {\"units\":\"degrees_east\",'standard_name':\"longitude\",'axis':\"X\"}\n",
    "\n",
    "    #flood=flood.rio.write_crs(\"epsg:4326\")\n",
    "\n",
    "    mergedfilepath=datadir+\"/\"+mergedfile\n",
    "\n",
    "    print(\"writing netcdf file: {}\".format(mergedfilepath))\n",
    "    ds.to_netcdf(mergedfilepath)\n",
    "    print(\"finished\")\n",
    "else:\n",
    "    print(\"No new files downloaded. Skipping updating merged file\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5381d497",
   "metadata": {},
   "source": [
    "# Downloading and preparing CHIRPS rainfall data\n",
    "this loads tif files for individual days, merges them and converts to netcdf format for easier ingestion in other scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84f3fcf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking monthly data between 2022-01-01 00:00:00 and 2022-10-18 06:45:59.608028\n"
     ]
    }
   ],
   "source": [
    "#endDate should be either date as YYYY-MM-DD or \"today\"\n",
    "endDate=\"2020-02-01\"\n",
    "endDate=\"today\"\n",
    "\n",
    "#either date as YYYY-MM-DD or number of days before the endDate\n",
    "startDate=\"2022-01-01\"\n",
    "\n",
    "\n",
    "remoteserver=\"https://data.chc.ucsb.edu\"\n",
    "remotedir_monthly=\"{}/products/CHIRPS-2.0/africa_monthly/tifs/\".format(remoteserver)\n",
    "remotedir_prelim=\"{}/products/CHIRPS-2.0/prelim/global_daily/tifs/p25/\".format(remoteserver)\n",
    "\n",
    "\n",
    "datadir=\"../data/rainfall/\"\n",
    "tifdir_monthly=datadir+\"/chirps-v2.0/tifs/\"\n",
    "filenamepattern_monthly=\"chirps-v2.0.{}.tif\"\n",
    "tifdir_prelim=datadir+\"/chirps-v2.0-prelim/tifs/\"\n",
    "filenamepattern_prelim=\"chirps-v2.0.{}.tif\"\n",
    "\n",
    "mergedfile=\"pr_mon_CHG_CHIRPS-2.0-0p25-prelim_merged_okavango.nc\"\n",
    "mergedfilepath=\"{}/{}\".format(datadir, mergedfile)\n",
    "\n",
    "\n",
    "minlon,maxlon=15,26\n",
    "minlat,maxlat=-11,-22\n",
    "\n",
    "\n",
    "verbose=True\n",
    "\n",
    "##########################################################################################################\n",
    "try:\n",
    "    endDate=datetime.datetime.strptime(endDate, \"%Y-%m-%d\")\n",
    "except:\n",
    "    endDate=datetime.datetime.today() #today\n",
    "\n",
    "try:\n",
    "    startDate=datetime.datetime.strptime(startDate, \"%Y-%m-%d\")\n",
    "except:\n",
    "    if not isinstance(startDate,int):\n",
    "        #this should not be lower than 50 - this is because chirps gets updated in monthly batches,\n",
    "        # the entire month gets updates on the 16th of the next month, so on the 16th we need to check for\n",
    "        # and update data since the beginnng of the previous month\n",
    "        startDate=60 \n",
    "    startDate=endDate - datetime.timedelta(startDate)\n",
    "\n",
    "if endDate<=startDate:\n",
    "    print(\"startDate has to be before endDate. You got: \\nstartDate:{} \\nendDate:{}. \\nExiting...\".format(startDate,endDate))\n",
    "    sys.exit()\n",
    "\n",
    "print(\"checking monthly data between {} and {}\".format(startDate,endDate))\n",
    "dates2check_monthly=pd.date_range(startDate.strftime(\"%Y-%m-%d\"),endDate.strftime(\"%Y-%m-%d\"), freq=\"M\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acff0275",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(_url,_localfilepath, verbose=True):\n",
    "    response=requests.head(_url)\n",
    "    if response.status_code!=200:\n",
    "        if verbose:\n",
    "            print(\" {} does not exist. skipping...\".format(_url))\n",
    "        return False\n",
    "    else:\n",
    "        with requests.get(_url, stream=True) as r:\n",
    "            r.raise_for_status()\n",
    "            with open(_localfilepath, 'wb') as f:\n",
    "                for chunk in r.iter_content(chunk_size=8192): \n",
    "                    f.write(chunk)\n",
    "\n",
    "        localfilesize=os.stat(_localfilepath)[6]\n",
    "        remotefilesize=int(response.headers['content-length'])\n",
    "        if verbose:\n",
    "            print (\"downloaded file size: {}\".format(localfilesize))\n",
    "            print (\"expected file size: {}\".format(remotefilesize))\n",
    "        if localfilesize != remotefilesize:\n",
    "            if verbose:\n",
    "                print(\"something went wrong. removing downloaded file\")\n",
    "            os.rename(_localfilepath, _localfilepath+\".fail\")\n",
    "            return False\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(\"download successful\")\n",
    "            return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67c8424c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking if https://data.chc.ucsb.edu is up\n",
      "it is up\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "update=False\n",
    "\n",
    "#checking if server is up\n",
    "print(\"checking if {} is up\".format(remoteserver))\n",
    "cont=False\n",
    "try:\n",
    "    response=requests.head(remoteserver)\n",
    "    response.raise_for_status()\n",
    "    cont=True\n",
    "    print(\"it is up\")\n",
    "except requests.exceptions.HTTPError as errh:\n",
    "    print (\"Http Error:\",errh)\n",
    "except requests.exceptions.ConnectionError as errc:\n",
    "    print (\"Error Connecting:\",errc)\n",
    "except requests.exceptions.Timeout as errt:\n",
    "    print (\"Timeout Error:\",errt)\n",
    "except requests.exceptions.RequestException as err:\n",
    "    print (\"Oops: Something Else\",err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6243e1dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chirps-v2.0.2022.01.tif\n",
      "file chirps-v2.0.2022.01.tif exists locally. skipping...\n",
      "chirps-v2.0.2022.02.tif\n",
      "file chirps-v2.0.2022.02.tif exists locally. skipping...\n",
      "chirps-v2.0.2022.03.tif\n",
      "file chirps-v2.0.2022.03.tif exists locally. skipping...\n",
      "chirps-v2.0.2022.04.tif\n",
      "file chirps-v2.0.2022.04.tif exists locally. skipping...\n",
      "chirps-v2.0.2022.05.tif\n",
      "file chirps-v2.0.2022.05.tif exists locally. skipping...\n",
      "chirps-v2.0.2022.06.tif\n",
      "file chirps-v2.0.2022.06.tif exists locally. skipping...\n",
      "chirps-v2.0.2022.07.tif\n",
      "file chirps-v2.0.2022.07.tif exists locally. skipping...\n",
      "chirps-v2.0.2022.08.tif\n",
      "file chirps-v2.0.2022.08.tif exists locally. skipping...\n",
      "chirps-v2.0.2022.09.tif\n",
      "file chirps-v2.0.2022.09.tif exists locally. skipping...\n"
     ]
    }
   ],
   "source": [
    "if cont:\n",
    "    #server is up\n",
    "    missingdays=0\n",
    "    for mdate in dates2check_monthly:\n",
    "    #https://data.chc.ucsb.edu/products/CHIRPS-2.0/africa_monthly/tifs/chirps-v2.0.1981.01.tif.gz\n",
    "        file_monthly=filenamepattern_monthly.format(mdate.strftime(\"%Y.%m\"))\n",
    "        print(file_monthly)\n",
    "        gzipfile_monthly=\"{}.gz\".format(file_monthly)\n",
    "        localfilepath_monthly=tifdir_monthly+\"/\"+file_monthly\n",
    "        localgzipfilepath_monthly=tifdir_monthly+\"/\"+gzipfile_monthly\n",
    "        if os.path.exists(localfilepath_monthly):\n",
    "            if verbose:\n",
    "                cont=True\n",
    "                print(\"file {} exists locally. skipping...\".format(file_monthly))\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(\"Checking if gzip file {} exists\".format(gzipfile_monthly))\n",
    "            if os.path.exists(localgzipfilepath_monthly):\n",
    "                if verbose:\n",
    "                    print(\"gzip file exists. unzipping...\")\n",
    "                with gzip.open(localgzipfilepath_monthly) as f_in:\n",
    "                    with open(localfilepath_monthly, 'wb') as f_out:\n",
    "                        shutil.copyfileobj(f_in, f_out)\n",
    "            else:\n",
    "                if verbose:\n",
    "                    print(\"Downloading {} from {} into {}\".format(gzipfile_monthly,remotedir_monthly,tifdir_monthly))\n",
    "                url=\"{}/{}\".format(remotedir_monthly,gzipfile_monthly)\n",
    "                response=download_file(url,localgzipfilepath_monthly)\n",
    "                if response:\n",
    "                    if verbose:\n",
    "                        print(\"unzipping {} into {}\".format(gzipfile_monthly,file_monthly))\n",
    "                    with gzip.open(localgzipfilepath_monthly) as f_in:\n",
    "                        with open(localfilepath_monthly, 'wb') as f_out:\n",
    "                            shutil.copyfileobj(f_in, f_out)\n",
    "                else:\n",
    "                    #downloading prelim file\n",
    "                    missingdays = pd.Period(date.strftime(\"%Y.%m.%d\")).days_in_month\n",
    "                    for day in range(1,missingdays+1):\n",
    "                        prelimfile=prelimfilenamepattern.format(date.strftime(\"%Y.%m.{}\".format(str(day).zfill(2))))\n",
    "                        prelimlocalfilepath=prelimtifdir+\"/\"+prelimfile\n",
    "                        if os.path.exists(prelimlocalfilepath):\n",
    "                            if verbose:\n",
    "                                print(\"file {} exists locally. skipping...\".format(prelimfile))\n",
    "                        else:\n",
    "                            if verbose:\n",
    "                                print(\"Downloading {}/{}\".format(prelimremotedir,prelimfile))\n",
    "                            url=\"{}/{}/{}\".format(prelimremotedir,date.strftime(\"%Y\"),prelimfile)\n",
    "                            response2=download_file(url,prelimlocalfilepath)\n",
    "            \n",
    "else:\n",
    "    print(\"server {} is down.\".format(remoteserver))\n",
    "    update=True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4674de02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 501 files in ../data/rainfall//chirps-v2.0/tifs/\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "files=glob.glob(\"{}/{}\".format(tifdir_monthly,filenamepattern_monthly.format(\"*\")))\n",
    "print(\"processing {} files in {}\".format(len(files), tifdir_monthly))\n",
    "\n",
    "i=0\n",
    "for file in np.sort(files):\n",
    "    i=i+1\n",
    "    if i%100==0:\n",
    "        print(i)\n",
    "    filedatestr=os.path.basename(file)[12:19]\n",
    "    filedate=datetime.datetime.strptime(filedatestr, '%Y.%m')\n",
    "    data=xr.open_dataset(file).band_data[0,:,:].sel(x=slice(minlon,maxlon), y=slice(minlat,maxlat)).expand_dims(time=[filedate])\n",
    "    data.name=\"pr\"\n",
    "    if i==1:\n",
    "        ds=data.copy(deep=True)\n",
    "    else:\n",
    "        ds=xr.concat([ds,data], dim=\"time\")\n",
    "#        sys.exit()\n",
    "        \n",
    "\n",
    "ds = ds.coarsen(x=5, y=5, boundary='pad').mean()\n",
    "ds=ds.assign_coords({\"y\":np.round(ds.y.data,3), \"x\":np.round(ds.x.data,3)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef82f619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "monthly data available for the entire period. No need to process prelim data. Skipping...\n"
     ]
    }
   ],
   "source": [
    "if missingdays>0:\n",
    "    files=glob.glob(\"{}/{}\".format(tifdir_prelim,filenamepattern_prelim.format(mdate.strftime(\"%Y.%m.*\"))))\n",
    "    print(\"processing {} files in {}\".format(len(files), tifdir_prelim))\n",
    "\n",
    "    if len(files)==ndays:\n",
    "        for j in range(ndays):\n",
    "            filedatestr=mdate.strftime(\"%Y.%m.{}\".format(str(j+1).zfill(2)))\n",
    "            filedate=datetime.datetime.strptime(filedatestr, '%Y.%m.%d')\n",
    "            file=\"{}/{}\".format(tifdir_prelim,filenamepattern_prelim.format(filedatestr))\n",
    "            data=xr.open_dataset(file).band_data[0,:,:].sel(x=slice(minlon,maxlon), y=slice(minlat,maxlat)).expand_dims(time=[filedate])\n",
    "            if j==0:\n",
    "                dsp=data.copy(deep=True)\n",
    "            else:\n",
    "                dsp=xr.concat([dsp,data], dim=\"time\")\n",
    "        dsp=dsp.sum(\"time\").expand_dims(time=[mdate])\n",
    "        print(\"merging monthly and prelim files\")\n",
    "        ds=xr.concat([ds,dsp],dim=\"time\")\n",
    "        ds=ds.rename({\"x\":\"longitude\",\"y\":\"latitude\"})\n",
    "    else:\n",
    "        print(\"month has {} days, but there are only {} available. skipping..\".format(ndays,len(files)))\n",
    "else:\n",
    "    print(\"monthly data available for the entire period. No need to process prelim data. Skipping...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eb285a1b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing merged netcdf file pr_mon_CHG_CHIRPS-2.0-0p25-prelim_merged_okavango.nc\n",
      "(501, 44, 44)\n"
     ]
    }
   ],
   "source": [
    "print(\"writing merged netcdf file {}\".format(mergedfile))\n",
    "if os.path.exists(mergedfilepath):\n",
    "    os.remove(mergedfilepath)\n",
    "print(ds.shape)\n",
    "ds.to_netcdf(mergedfilepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0397cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
